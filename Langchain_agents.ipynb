{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSOuEqV50rV3",
        "outputId": "609bcf23-7604-40a1-a2d9-59e6ad0008a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.0 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain -qU\n",
        "!pip install langchain_community -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TOPkgKtu1RJd",
        "outputId": "f36b8a04-2790-44b3-8e88-97f46c084005"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.72)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.8-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "901f59ca817b48b3a75d826388715925"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set OpenAI API key\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "f-QJ34Fg1bFB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAJOk-uR2Wl9",
        "outputId": "c3f8f894-f24a-40b4-c471-ef4a217d7543"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.1.1 primp-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U duckduckgo-search\n",
        "\n",
        "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\n",
        "\n",
        "# Web Search Tool using DuckDuckGo\n",
        "search_tool = Tool(\n",
        "    name=\"DuckDuckGo Search\",\n",
        "    func=DuckDuckGoSearchRun().run,\n",
        "    description=\"Use this tool to perform web searches using DuckDuckGo.\"\n",
        ")\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "tools = [search_tool]\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "cHG1H3ft4Zae"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gHDll_F4zW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fbdbe04",
        "outputId": "fca45afb-37ac-4fe6-db2e-3bd19e7f4309"
      },
      "source": [
        "response = agent.run(\"What are the top trending movies this week?\")\n",
        "\n",
        "print(response)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. This information changes weekly, so I will use the DuckDuckGo Search tool to get the most up-to-date information from reliable sources like movie review sites or entertainment news.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"top trending movies this week\"\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe meaning of TOP is the highest point, level, or part of something : summit, crown. How to use top in a sentence. TOP definition: 1. the highest place or part: 2. the flat upper surface of something: 3. in baseball, the first…. Learn more. The domain name extension .top is managed and operated by the .top registry (registry backend ZDNS) which belongs to Jiangsu Bangning Science & Technology Co., Ltd. in Nanjing, China, … 5 days ago · top (countable and uncountable, plural tops) The highest or uppermost part of something. Synonyms: peak, summit, overside Antonyms: bottom, base, underside His kite … Experience a seamless connection between Top Hat and your LMS. Enjoy easy navigation, direct links to course materials, and synced grades for better teaching and learning.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. The previous search was not specific enough. I will try a more targeted search to find box office results or streaming charts, which are good indicators of trending movies.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"weekend box office results\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNo good DuckDuckGo Search Result was found\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. The previous searches were not successful. I will try a more specific search query to find a reliable source for movie trends, such as a well-known movie database or entertainment news website.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"most popular movies right now\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNo good DuckDuckGo Search Result was found\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. My previous attempts were not specific enough. I will try a more targeted search to find a reliable source that tracks movie popularity, such as a well-known movie database or entertainment news website.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"IMDb most popular movies\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m1 day ago · Film topics not covered in the boards below. 4 days ago · IMDB v2.3 Internet Media Discussion Boards Home Help Rules Search Welcome Guest. Please Login or Register. Mar 25, 2025 · Discuss films currently in theaters and those coming soon. 4 days ago · Discussion forum for various topics and interests. Dec 8, 2024 · What movies are you looking forward to next year? For me: Wolf Man - The trailer looks really good. Leigh Whanell might be one of the best genre filmmakers working today N\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. My previous searches have not yielded a direct list. I will try a more specific search query targeting a well-known movie review and tracking website.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"Rotten Tomatoes top movies this week\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNo good DuckDuckGo Search Result was found\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. My previous searches have been unsuccessful. I will try a more specific search query to find a reliable source for movie trends, such as a well-known movie database or entertainment news website.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"current top box office movies\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m3 days ago · Box Office Mojo and IMDb are trademarks or registered trademarks of IMDb.com, Inc. or its affiliates. Conditions of Use and Privacy Policy under which this service is provided … Weekend Gross: $1.3M Total Gross: $69M Weeks Released: 5 7.1 (70K) Rate Mark as watched Reported by Box Office Mojo © 2025 See more box office results at BoxOfficeMojo.com Rotten Tomatoes, home of the Tomatometer, is the most trusted measurement of quality for Movies & TV. The definitive site for Reviews, Trailers, Showtimes, and Tickets. Jul 18, 2025 · North American Box Office results for this weekend. Jul 18, 2025 · The top movies in the most recent box office results.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. The last search for \"current top box office movies\" gave a promising result mentioning Box Office Mojo. I will now perform a more specific search to get the current list from that source.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"Box Office Mojo weekend box office\"\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mSign In to Your Account. Email Address. Next. Reset Password. or. Sign in with Google. ©2025 Box . Privacy Policy. Terms. Help. By investing in a cloud content management platform like Box and leveraging other best-of-breed technology partners, we have been able to create a more secure, efficient, and collaborative environment for conducting business. Feb 26, 2020 · Solve issues and optimize your workflow with our help. Get expert support for Box, the leading cloud intelligent content management platform. Shop from a wide selection of boxes to suit your shipping, moving and packaging needs 4 days ago · Box helps you get work done on the go. It's fast, secure and simple to use, so you can be productive from anywhere, which is the reason 57,000 businesses, including Eli Lilly and Company, General Electric, KKR & Co., P&G and The GAP securely access and manage their critical information with Box.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to find out which movies are currently trending. My previous searches have been unsuccessful in providing a direct list. I will try a more specific search query to find a reliable source for movie trends, such as a well-known movie database or entertainment news website.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: \"Box Office Mojo\"\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m2 days ago · Forced to balance their roles as heroes with the strength of their family bond, the Fantastic Four must defend Earth from a ravenous space god called Galactus and his … Jul 21, 2025 · With Superman and Jurassic World: Rebirth leading the box office, their continued momentum and audience draw have become key industry highlights. Superman has soared … 1 day ago · Marvel Studios' \"The Fantastic Four: First Steps\" took the top spot at the box office this weekend, knocking DC Studios' \"Superman\" to second place. 1 day ago · Box Office Mojo and IMDb are trademarks or registered trademarks of IMDb.com, Inc. or its affiliates. Conditions of Use and Privacy Policy under which this service is provided to you. 1 day ago · Latest Weekend: Jul 25-27More »\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: According to Box Office Mojo, the top trending movies at the box office for the weekend of July 25-27 are \"The Fantastic Four: First Steps\" and \"Superman\".\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "According to Box Office Mojo, the top trending movies at the box office for the weekend of July 25-27 are \"The Fantastic Four: First Steps\" and \"Superman\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The WikipediaAPIWrapper - allows you to query Wikipedia for articles and summaries.\n",
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC3mskRc5n1a",
        "outputId": "e2e108dd-030a-4a58-c46e-38be600e89e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.14.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=d81126a70374e5310a5f3a7b135376968f76c5748567cafc43af725a6263b369\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "\n",
        "# Step 1: Initialize the Wikipedia Tool\n",
        "wikipedia_tool = Tool(\n",
        "    name=\"Wikipedia Search\",\n",
        "    func=WikipediaAPIWrapper().run,\n",
        "    description=\"Use this tool to search Wikipedia for information on any topic.\"\n",
        ")\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "tools = [wikipedia_tool]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u4JauHuh5z25"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "200308c4",
        "outputId": "18371116-b8f1-43a1-a45f-f8b8b9f16961"
      },
      "source": [
        "response = agent.run(\"Who is Albert einstin, and what are his contributions?\")\n",
        "\n",
        "print(response)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought\n",
            "The user is asking for information about Albert Einstein and his contributions. I will use the Wikipedia Search tool to find this information. The name is misspelled as \"einstin\", but the search tool should be able to handle this common misspelling.\n",
            "\n",
            "Action: Wikipedia Search\n",
            "Action Input: Albert Einstein\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Albert Einstein\n",
            "Summary: Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is best known for developing the theory of relativity. Einstein also made important contributions to quantum mechanics. His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called \"the world's most famous equation\". He received the 1921 Nobel Prize in Physics for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect.\n",
            "Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg) the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zurich, graduating in 1900. He acquired Swiss citizenship a year later, which he kept for the rest of his life, and afterwards secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin to join the Prussian Academy of Sciences and the Humboldt University of Berlin, becoming director of the Kaiser Wilhelm Institute for Physics in 1917; he also became a German citizen again, this time as a subject of the Kingdom of Prussia. In 1933, while Einstein was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi persecution of his fellow Jews, he decided to remain in the US, and was granted American citizenship in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research.\n",
            "In 1905, sometimes described as his annus mirabilis (miracle year), he published four groundbreaking papers. In them, he outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole. In 1917, Einstein wrote a paper which introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which contained a trove of information that would be beneficial to developments in physics later on, such as quantum electrodynamics and quantum optics.\n",
            "In the middle part of his career, Einstein made important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. With physicist Satyendra Nath Bose, he laid the groundwork for Bose–Einstein statistics. For much of the last phase of his academic life, Einstein worked on two endeavors that ultimately proved unsuccessful. First, he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that God does not play dice. Second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism. As a result, he became increasingly isolated from mainstream modern physics.\n",
            "\n",
            "Page: Hans Albert Einstein\n",
            "Summary: Hans Albert Einstein (May 14, 1904 – July 26, 1973) was a Swiss-American engineer, the second child and first son of physicists Albert Einstein and Mileva Marić. He was a long-time professor of hydraulic engineering at the University of California, Berkeley.\n",
            "Einstein was widely recognized for his research on sediment transport. To honor \u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:The user asked for information about \"Albert einstin\". The Wikipedia search for \"Albert Einstein\" returned a comprehensive summary of his life and major contributions. The summary covers his key theories (relativity, quantum mechanics), his famous equation (E=mc²), his Nobel Prize, and his personal history. This is sufficient to answer the user's question.\n",
            "\n",
            "I will structure the answer by first identifying who he was and then listing his major contributions based on the provided text.\n",
            "1.  **Who he was:** A German-born theoretical physicist.\n",
            "2.  **Contributions:**\n",
            "    *   Developed the theory of relativity (both special and general).\n",
            "    *   Made important contributions to quantum mechanics.\n",
            "    *   Formulated the mass-energy equivalence formula, E = mc².\n",
            "    *   Discovered the law of the photoelectric effect, for which he won the 1921 Nobel Prize in Physics.\n",
            "    *   Published four groundbreaking papers in his \"miracle year\" (1905) on the photoelectric effect, Brownian motion, special relativity, and mass-energy equivalence.\n",
            "    *   Introduced concepts of spontaneous and stimulated emission, which are fundamental to lasers.\n",
            "    *   Worked with Satyendra Nath Bose on Bose-Einstein statistics.I now know the final answer\n",
            "Final Answer:Albert Einstein (1879-1955) was a German-born theoretical physicist, widely regarded as one of the most influential scientists of all time.\n",
            "\n",
            "His major contributions to science include:\n",
            "*   **The Theory of Relativity:** He developed both the special theory of relativity and the general theory of relativity, which fundamentally changed our understanding of space, time, gravity, and the universe.\n",
            "*   **Mass-Energy Equivalence:** From his theory of special relativity, he derived the famous equation E = mc², which demonstrates that mass and energy are equivalent and can be converted into one another.\n",
            "*   **The Photoelectric Effect:** He received the 1921 Nobel Prize in Physics for his explanation of the photoelectric effect, which was a pivotal step in the development of quantum theory.\n",
            "*   **Quantum Mechanics:** Einstein made significant early contributions to quantum theory, including the concept of light quanta (photons) and the concepts of spontaneous and stimulated emission, which form the basis for lasers.\n",
            "*   **Brownian Motion:** His 1905 paper explaining Brownian motion provided empirical evidence for the existence of atoms.\n",
            "*   **Bose-Einstein Statistics:** In collaboration with Satyendra Nath Bose, he developed a statistical description of a class of particles, now known as bosons.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Albert Einstein (1879-1955) was a German-born theoretical physicist, widely regarded as one of the most influential scientists of all time.\n",
            "\n",
            "His major contributions to science include:\n",
            "*   **The Theory of Relativity:** He developed both the special theory of relativity and the general theory of relativity, which fundamentally changed our understanding of space, time, gravity, and the universe.\n",
            "*   **Mass-Energy Equivalence:** From his theory of special relativity, he derived the famous equation E = mc², which demonstrates that mass and energy are equivalent and can be converted into one another.\n",
            "*   **The Photoelectric Effect:** He received the 1921 Nobel Prize in Physics for his explanation of the photoelectric effect, which was a pivotal step in the development of quantum theory.\n",
            "*   **Quantum Mechanics:** Einstein made significant early contributions to quantum theory, including the concept of light quanta (photons) and the concepts of spontaneous and stimulated emission, which form the basis for lasers.\n",
            "*   **Brownian Motion:** His 1905 paper explaining Brownian motion provided empirical evidence for the existence of atoms.\n",
            "*   **Bose-Einstein Statistics:** In collaboration with Satyendra Nath Bose, he developed a statistical description of a class of particles, now known as bosons.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Connect to SQLite database (creates example.db if it doesn't exist)\n",
        "connection = sqlite3.connect(\"example.db\")\n",
        "cursor = connection.cursor()\n",
        "\n",
        "# Create the inventory table\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS inventory (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    product_name TEXT NOT NULL,\n",
        "    stock_quantity INTEGER NOT NULL\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Insert sample data\n",
        "cursor.executemany(\"\"\"\n",
        "INSERT INTO inventory (product_name, stock_quantity) VALUES (?, ?)\n",
        "\"\"\", [\n",
        "    (\"Product A\", 150),\n",
        "    (\"Product B\", 200),\n",
        "    (\"Product C\", 300),\n",
        "    (\"Product D\", 100),\n",
        "    (\"Product E\", 50)\n",
        "])\n",
        "# Commit and close connection\n",
        "connection.commit()\n",
        "connection.close()\n",
        "\n",
        "print(\"Database created and populated with sample data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhDdSJwD_vPg",
        "outputId": "086a17ff-469d-4e00-a8f6-b0721853282f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database created and populated with sample data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# Step 1: Connect to the Database\n",
        "db = SQLDatabase.from_uri(\"sqlite:////content/example.db\")\n",
        "\n",
        "# Step 3: Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Step 2: Create the SQL Database Toolkit\n",
        "\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "\n",
        "tools = toolkit.get_tools()\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S70-mXuARr7",
        "outputId": "f5ccd52f-8df6-4969-eed0-247e5fd4175d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"How many units of Product A are in stock?\"\n",
        "response1 = agent.run(query1)\n",
        "print(\"Response:\", response1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXmu58vAsj8",
        "outputId": "2a408d0f-d226-473e-8278-e7afadb29c6e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find out how many units of Product A are in stock. First, I need to find the tables in the database to see which ones are relevant to products and stock.\n",
            "Action: sql_db_list_tables\n",
            "Action Input:\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3minventory\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe user wants to know the stock of Product A. I have found a table named 'inventory'. I should check the schema of this table to see if it contains product and stock information.\n",
            "Action: sql_db_schema\n",
            "Action Input: inventory\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m\n",
            "CREATE TABLE inventory (\n",
            "\tid INTEGER, \n",
            "\tproduct_name TEXT NOT NULL, \n",
            "\tstock_quantity INTEGER NOT NULL, \n",
            "\tPRIMARY KEY (id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from inventory table:\n",
            "id\tproduct_name\tstock_quantity\n",
            "1\tProduct A\t150\n",
            "2\tProduct B\t200\n",
            "3\tProduct C\t300\n",
            "*/\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe user wants to know the stock quantity of 'Product A'. The `inventory` table contains the columns `product_name` and `stock_quantity`. I can query this table to find the `stock_quantity` where the `product_name` is 'Product A'.\n",
            "Action: sql_db_query_checker\n",
            "Action Input: SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m```sql\n",
            "SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'\n",
            "```\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe user wants to know the stock of Product A.\n",
            "I have already identified the `inventory` table and its schema.\n",
            "The schema is:\n",
            "CREATE TABLE inventory (\n",
            "id INTEGER,\n",
            "product_name TEXT NOT NULL,\n",
            "stock_quantity INTEGER NOT NULL,\n",
            "PRIMARY KEY (id)\n",
            ")\n",
            "I have formulated a query to get the stock quantity for 'Product A': `SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'`.\n",
            "I have checked the query using `sql_db_query_checker` and it is correct.\n",
            "Now I need to execute the query using `sql_db_query` to get the result.\n",
            "Action: sql_db_query\n",
            "Action Input: SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[(150,)]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: There are 150 units of Product A in stock.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response: There are 150 units of Product A in stock.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ArxivQueryRun"
      ],
      "metadata": {
        "id": "iT_JVyZTBLVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28o8dmpDBjFz",
        "outputId": "352e02cf-16c6-49d8-f02a-b8fa49cee765"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.7.14)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=570884f1088b4c12105057404b680c3a6b8853589a0a61f0c60a5aa5266ba0e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "arxiv_tool = Tool(\n",
        "    name=\"Arxiv Search\",\n",
        "    func=ArxivQueryRun().run,\n",
        "    description=\"Use this tool to search academic papers on arXiv. Provide topics, authors, or keywords.\"\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Step 3: Combine the tool into a list\n",
        "tools = [arxiv_tool]\n",
        "\n",
        "# Step 4: Initialize the Agent\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")"
      ],
      "metadata": {
        "id": "cRr02WbGBMCn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Query the Agent\n",
        "query = \"Search for research papers on multimodel eye disease prediction model in 2024.\"\n",
        "response = agent.run(query)\n",
        "print(\"Response from Agent:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgtHqT0DBQsF",
        "outputId": "75a0f330-c985-4e51-eb0b-24538c90dd09"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The user is asking for research papers from 2024 on the topic of \"multimodal eye disease prediction models\". I should use the Arxiv Search tool to find relevant academic papers. I will use a search query that combines these keywords.\n",
            "Action: Arxiv Search\n",
            "Action Input: 'multimodal eye disease prediction 2024'\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/arxiv.py:105: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  ).results()\n",
            "<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-05-28\n",
            "Title: Confidence-aware multi-modality learning for eye disease screening\n",
            "Authors: Ke Zou, Tian Lin, Zongbo Han, Meng Wang, Xuedong Yuan, Haoyu Chen, Changqing Zhang, Xiaojing Shen, Huazhu Fu\n",
            "Summary: Multi-modal ophthalmic image classification plays a key role in diagnosing\n",
            "eye diseases, as it integrates information from different sources to complement\n",
            "their respective performances. However, recent improvements have mainly focused\n",
            "on accuracy, often neglecting the importance of confidence and robustness in\n",
            "predictions for diverse modalities. In this study, we propose a novel\n",
            "multi-modality evidential fusion pipeline for eye disease screening. It\n",
            "provides a measure of confidence for each modality and elegantly integrates the\n",
            "multi-modality information using a multi-distribution fusion perspective.\n",
            "Specifically, our method first utilizes normal inverse gamma prior\n",
            "distributions over pre-trained models to learn both aleatoric and epistemic\n",
            "uncertainty for uni-modality. Then, the normal inverse gamma distribution is\n",
            "analyzed as the Student's t distribution. Furthermore, within a\n",
            "confidence-aware fusion framework, we propose a mixture of Student's t\n",
            "distributions to effectively integrate different modalities, imparting the\n",
            "model with heavy-tailed properties and enhancing its robustness and\n",
            "reliability. More importantly, the confidence-aware multi-modality ranking\n",
            "regularization term induces the model to more reasonably rank the noisy\n",
            "single-modal and fused-modal confidence, leading to improved reliability and\n",
            "accuracy. Experimental results on both public and internal datasets demonstrate\n",
            "that our model excels in robustness, particularly in challenging scenarios\n",
            "involving Gaussian noise and modality missing conditions. Moreover, our model\n",
            "exhibits strong generalization capabilities to out-of-distribution data,\n",
            "underscoring its potential as a promising solution for multimodal eye disease\n",
            "screening.\n",
            "\n",
            "Published: 2024-05-22\n",
            "Title: EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging\n",
            "Authors: Danli Shi, Weiyi Zhang, Xiaolan Chen, Yexin Liu, Jiancheng Yang, Siyu Huang, Yih Chung Tham, Yingfeng Zheng, Mingguang He\n",
            "Summary: Artificial intelligence (AI) is vital in ophthalmology, tackling tasks like\n",
            "diagnosis, classification, and visual question answering (VQA). However,\n",
            "existing AI models in this domain often require extensive annotation and are\n",
            "task-specific, limiting their clinical utility. While recent developments have\n",
            "brought about foundation models for ophthalmology, they are limited by the need\n",
            "to train separate weights for each imaging modality, preventing a comprehensive\n",
            "representation of multi-modal features. This highlights the need for versatile\n",
            "foundation models capable of handling various tasks and modalities in\n",
            "ophthalmology. To address this gap, we present EyeFound, a multimodal\n",
            "foundation model for ophthalmic images. Unlike existing models, EyeFound learns\n",
            "generalizable representations from unlabeled multimodal retinal images,\n",
            "enabling efficient model adaptation across multiple applications. Trained on\n",
            "2.78 million images from 227 hospitals across 11 ophthalmic modalities,\n",
            "EyeFound facilitates generalist representations and diverse multimodal\n",
            "downstream tasks, even for detecting challenging rare diseases. It outperforms\n",
            "previous work RETFound in diagnosing eye diseases, predicting systemic disease\n",
            "incidents, and zero-shot multimodal VQA. EyeFound provides a generalizable\n",
            "solution to improve model performance and lessen the annotation burden on\n",
            "experts, facilitating widespread clinical AI applications for retinal imaging.\n",
            "\n",
            "Published: 2023-11-26\n",
            "Title: Eye Disease Prediction using Ensemble Learning and Attention on OCT Scans\n",
            "Authors: Gauri Naik, Nandini Narvekar, Dimple Agarwal, Nishita Nandanwar, Himangi Pande\n",
            "Summary: Eye diseases have posed significant challenges for decades, but advancements\n",
            "in technology have opened new avenues for their detection and treatment.\n",
            "Machine learnin\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mHere are some research papers on multimodal eye disease prediction models published in 2024:\n",
            "\n",
            "**1. Confidence-aware multi-modality learning for eye disease screening**\n",
            "\n",
            "*   **Authors:** Ke Zou, Tian Lin, Zongbo Han, Meng Wang, Xuedong Yuan, Haoyu Chen, Changqing Zhang, Xiaojing Shen, Huazhu Fu\n",
            "*   **Publication Date:** May 28, 2024\n",
            "*   **Summary:** This paper introduces a novel pipeline for multi-modal eye disease screening that focuses on the confidence and robustness of predictions. The proposed method uses a confidence-aware fusion framework to integrate information from different modalities, which enhances the model's reliability, especially in challenging scenarios with noisy or missing data. The model not only provides a diagnosis but also a measure of confidence in its prediction.\n",
            "\n",
            "**2. EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging**\n",
            "\n",
            "*   **Authors:** Danli Shi, Weiyi Zhang, Xiaolan Chen, Yexin Liu, Jiancheng Yang, Siyu Huang, Yih Chung Tham, Yingfeng Zheng, Mingguang He\n",
            "*   **Publication Date:** May 22, 2024\n",
            "*   **Summary:** This research presents \"EyeFound,\" a large-scale, multimodal foundation model for ophthalmic images. Trained on 2.78 million images across 11 different modalities, EyeFound is designed to be a generalist model that can be adapted for various downstream tasks, including disease diagnosis, systemic disease prediction, and visual question answering. It aims to reduce the need for extensive annotation and improve model performance across a wide range of clinical applications.\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: Here are two research papers on multimodal eye disease prediction models published in 2024:\n",
            "\n",
            "1.  **Title:** Confidence-aware multi-modality learning for eye disease screening\n",
            "    *   **Authors:** Ke Zou, Tian Lin, Zongbo Han, Meng Wang, Xuedong Yuan, Haoyu Chen, Changqing Zhang, Xiaojing Shen, Huazhu Fu\n",
            "    *   **Published:** May 28, 2024\n",
            "    *   **Summary:** This paper proposes a new method for multi-modal eye disease screening that emphasizes the confidence and robustness of its predictions. It uses a novel fusion technique to combine data from different sources, which improves the model's reliability, especially when dealing with noisy or incomplete data. The model is designed to be more robust and reliable for clinical use.\n",
            "\n",
            "2.  **Title:** EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging\n",
            "    *   **Authors:** Danli Shi, Weiyi Zhang, Xiaolan Chen, Yexin Liu, Jiancheng Yang, Siyu Huang, Yih Chung Tham, Yingfeng Zheng, Mingguang He\n",
            "    *   **Published:** May 22, 2024\n",
            "    *   **Summary:** This research introduces \"EyeFound,\" a large-scale foundation model for ophthalmology. It was trained on 2.78 million images from 11 different ophthalmic modalities. Unlike task-specific models, EyeFound learns general representations from unlabeled images, allowing it to be adapted for a wide range of tasks, including disease diagnosis, predicting systemic diseases, and visual question answering, thereby reducing the need for extensive manual annotation.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response from Agent:\n",
            "Here are two research papers on multimodal eye disease prediction models published in 2024:\n",
            "\n",
            "1.  **Title:** Confidence-aware multi-modality learning for eye disease screening\n",
            "    *   **Authors:** Ke Zou, Tian Lin, Zongbo Han, Meng Wang, Xuedong Yuan, Haoyu Chen, Changqing Zhang, Xiaojing Shen, Huazhu Fu\n",
            "    *   **Published:** May 28, 2024\n",
            "    *   **Summary:** This paper proposes a new method for multi-modal eye disease screening that emphasizes the confidence and robustness of its predictions. It uses a novel fusion technique to combine data from different sources, which improves the model's reliability, especially when dealing with noisy or incomplete data. The model is designed to be more robust and reliable for clinical use.\n",
            "\n",
            "2.  **Title:** EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging\n",
            "    *   **Authors:** Danli Shi, Weiyi Zhang, Xiaolan Chen, Yexin Liu, Jiancheng Yang, Siyu Huang, Yih Chung Tham, Yingfeng Zheng, Mingguang He\n",
            "    *   **Published:** May 22, 2024\n",
            "    *   **Summary:** This research introduces \"EyeFound,\" a large-scale foundation model for ophthalmology. It was trained on 2.78 million images from 11 different ophthalmic modalities. Unlike task-specific models, EyeFound learns general representations from unlabeled images, allowing it to be adapted for a wide range of tasks, including disease diagnosis, predicting systemic diseases, and visual question answering, thereby reducing the need for extensive manual annotation.\n"
          ]
        }
      ]
    }
  ]
}